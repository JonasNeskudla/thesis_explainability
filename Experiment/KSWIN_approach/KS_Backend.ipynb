{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from scipy.spatial import distance\n",
    "from skmultiflow.data.data_stream import DataStream\n",
    "from skmultiflow.drift_detection import PageHinkley\n",
    "from skmultiflow.drift_detection.adwin import ADWIN\n",
    "from skmultiflow.drift_detection import KSWIN \n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, matthews_corrcoef, accuracy_score\n",
    "from scipy.stats import entropy as e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapDetector:\n",
    "    def __init__(self, base_detector_type = None, base_detector_object = None, base_detector_config = None):\n",
    "        # default base detector\n",
    "        self.base_detector = None\n",
    "        self.base_detector_type = None\n",
    "        \n",
    "        # ph default params\n",
    "        self.alpha = None\n",
    "        self.delta = None\n",
    "        self.min_instances = None\n",
    "        self.threshold = None\n",
    "        \n",
    "        # adwin standard params\n",
    "        self.ad_delta = None\n",
    "        \n",
    "        # kswin standard params\n",
    "        self.ks_alpha = None\n",
    "        \n",
    "        # performance metrics\n",
    "        self.auc = None\n",
    "        self.acc_score = None\n",
    "        self.weighted_f1_score = None\n",
    "        self.mcc_score = None\n",
    "        self.score_list = []\n",
    "        self.weighted_f1_list = []\n",
    "\n",
    "\n",
    "        # change statistics\n",
    "        self.shap_list = []\n",
    "        self.drift_detections = []\n",
    "        self.distances = []\n",
    "        self.p_values = []\n",
    "        self.prob_predictions = []\n",
    "        self.predictions = []\n",
    "        self.true_label_list = []\n",
    "        \n",
    "        # statitics\n",
    "        self.true_drift_points = []\n",
    "        self.al_percentage = None\n",
    "        self.sparsity = None\n",
    "        \n",
    "        # shap detector\n",
    "        self.retrainsize = None\n",
    "        self.samplesize = None\n",
    "        self.initial_batch_size = None\n",
    "        self.approach = None\n",
    "        \n",
    "        # error rate based detector\n",
    "        self.err_based = None\n",
    "        \n",
    "        # model type\n",
    "        self.model_type = None\n",
    "                \n",
    "        # active learning\n",
    "        self.samping = None\n",
    "            \n",
    "        # define statistical detector\n",
    "        if base_detector_object is not None:\n",
    "            self.base_detector = base_detector_object\n",
    "            print('Parameters of base detector object are not listed individually in the evaluation. For this purpose the configuration must be specified at the function call')\n",
    "\n",
    "        else: \n",
    "            if base_detector_type is None: \n",
    "                self.base_detector = self.create_base_detector()\n",
    "                    \n",
    "            elif base_detector_type == \"ph\":\n",
    "                self.base_detector = self.create_base_detector(base_detector_type = 'ph', base_detector_config = base_detector_config)\n",
    "            \n",
    "            elif base_detector_type == \"adwin\":\n",
    "                self.base_detector = self.create_base_detector(base_detector_type = 'adwin', base_detector_config = base_detector_config)\n",
    "                \n",
    "            elif base_detector_type == \"kswin\":\n",
    "                self.base_detector = self.create_base_detector(base_detector_type = 'kswin', base_detector_config = base_detector_config)\n",
    "                \n",
    "            else:\n",
    "                raise ValueError('You have to provide either a detector object or a detector type (\"ph\", \"adwin\" or \"kswin\") with appropriate configuration!')\n",
    "\n",
    "                \n",
    "    def active_learning(self, x_labeled, x_unlabeled, y_labeled, columns, data_full, uncertainty_threshold_in, amount_classes, clf, sampling, al_percentage_in): \n",
    "        # initialize\n",
    "        cols = list(range(1, columns+1))\n",
    "        cols = [str(x) for x in cols]\n",
    "        cols_full = cols + ['label']\n",
    "        data_full.columns = list(cols_full)\n",
    "        \n",
    "        updated_threshold = False\n",
    "        uncertainty_threshold = uncertainty_threshold_in\n",
    "        x_unlabeled = pd.DataFrame(x_unlabeled, columns = cols)  \n",
    "        num_ask_instances = int(np.around((len(x_unlabeled)*al_percentage_in/100), decimals=0)) #total amount of unlabeled instances to query\n",
    "        num_ask_instances_subset = int(np.ceil(num_ask_instances/10)) #batch size to query per AL iteration\n",
    "        num_already_ask = 0\n",
    "        cols_probas = ['probas_class'+str(c) for c in range(1,amount_classes+1)]\n",
    "        cols_probas_conf = cols_probas[:]+['Confidence']\n",
    "        \n",
    "        it = 0\n",
    "        less_classes = False\n",
    "        \n",
    "        while num_already_ask < num_ask_instances:              \n",
    "            # sort by least confident instances from unlabeled data\n",
    "            probabilities = clf.predict_proba(x_unlabeled.values)    \n",
    "                \n",
    "            # in case that fewer classes are predicted than are present in the data set\n",
    "            if probabilities.shape[1] != len(cols_probas):\n",
    "                diff = np.abs(len(cols_probas)-probabilities.shape[1])\n",
    "                for i in range(diff):\n",
    "                    probabilities = [np.concatenate((p, [0]),axis=0) for p in probabilities]\n",
    "                probabilities = np.vstack(probabilities)\n",
    "                less_classes = True    \n",
    "            \n",
    "            probas = pd.DataFrame(probabilities, columns=cols_probas)\n",
    "            \n",
    "            if sampling == 'margin':\n",
    "                probas['Confidence'] = self.get_confidence(np.array(clf.predict_proba(x_unlabeled.values)), sampling)\n",
    "            else:\n",
    "                probas['Confidence'] = self.get_confidence(np.array(clf.predict_proba(x_unlabeled.values)), sampling)\n",
    "                \n",
    "            x_unlabeled.reset_index(drop=True, inplace=True)\n",
    "            probas.reset_index(drop=True, inplace=True)            \n",
    "            x_unlabeled = pd.concat([x_unlabeled, probas], axis=1)\n",
    "            \n",
    "            # determine the most uncertain instances that have a confidence level lower than threshold\n",
    "            if sampling == 'margin':\n",
    "                x_unlabeled.sort_values(by=['Confidence'], ascending=True, inplace=True)\n",
    "                instances_ask = x_unlabeled[x_unlabeled['Confidence']< uncertainty_threshold].iloc[:num_ask_instances_subset,:]\n",
    "                num_already_ask += len(instances_ask)\n",
    "            else:\n",
    "                x_unlabeled.sort_values(by=['Confidence'], ascending=False, inplace=True)\n",
    "                instances_ask = x_unlabeled.iloc[:num_ask_instances_subset,:].copy() \n",
    "                num_already_ask += len(instances_ask)\n",
    "\n",
    "            # if too many instances were requested, overwrite the batch with exactly the right number of instances to meet the limit\n",
    "            if num_already_ask > num_ask_instances and sampling == 'margin':\n",
    "                num_already_ask = num_already_ask-len(instances_ask)\n",
    "                instances_ask = x_unlabeled[x_unlabeled['Confidence']< uncertainty_threshold].iloc[:num_ask_instances-num_already_ask,:]\n",
    "                num_already_ask += len(instances_ask)\n",
    "            elif num_already_ask > num_ask_instances and sampling != 'margin':\n",
    "                num_already_ask = num_already_ask-len(instances_ask)\n",
    "                instances_ask = x_unlabeled.iloc[:num_ask_instances-num_already_ask,:].copy()\n",
    "                num_already_ask += len(instances_ask)\n",
    "\n",
    "            # if no instances fall below threshold, update the threshold\n",
    "            if len(instances_ask) == 0 and num_already_ask < num_ask_instances and sampling == 'margin': \n",
    "                uncertainty_threshold += 0.05\n",
    "                updated_threshold = True\n",
    "                x_unlabeled.drop(labels = cols_probas_conf, axis=1, inplace=True)\n",
    "                continue\n",
    "            \n",
    "            # if threshold was updated and therefore labels were found, reset the threshold to its initial value\n",
    "            if len(instances_ask) !=0 and updated_threshold==True and sampling == 'margin':\n",
    "                updated_threshold = False\n",
    "                uncertainty_threshold = uncertainty_threshold_in\n",
    "  \n",
    "            x_unlabeled.drop(labels = cols_probas_conf, axis=1, inplace=True)  \n",
    "            instances_ask.drop(labels = cols_probas_conf, axis=1, inplace=True) \n",
    "            \n",
    "            # remove the requested instances from x_unlabeled\n",
    "            x_unlabeled = x_unlabeled.iloc[len(instances_ask):,:]\n",
    "            \n",
    "            # get labels for least confident instances\n",
    "            new_labels = self.get_labels(data_full, instances_ask.values)\n",
    "\n",
    "            # attach labeled instances to labeled data (x_labeled)\n",
    "            x_labeled = np.concatenate((x_labeled, instances_ask)) \n",
    "            y_labeled = np.concatenate((y_labeled, new_labels))\n",
    "            \n",
    "            if it == 0:\n",
    "                if amount_classes > 2: \n",
    "                    clf = xgb.XGBClassifier(objective = 'multi:softprob', num_class = amount_classes)\n",
    "                    clf.fit(x_labeled, y_labeled)\n",
    "                else:\n",
    "                    clf = xgb.XGBClassifier()\n",
    "                    clf.fit(x_labeled, y_labeled) \n",
    "                    \n",
    "            elif less_classes == False:\n",
    "                clf.fit(x_labeled, y_labeled)\n",
    "                \n",
    "            else:\n",
    "                clf.fit(x_labeled, y_labeled)\n",
    "                less_classes = False\n",
    "\n",
    "            it += 1\n",
    "        return x_labeled, y_labeled\n",
    "    \n",
    "    \n",
    "    def detect_drift(self, fix_drifts, data_sparse, data_full, sparsity, initial_batch_size, initial_batch_sample, samplesize, retrainsize, distance_measure, approach, al_percentage, uncertainty_threshold, true_drift_points, err_based, multiclass, amount_classes, real_world, sampling, clf=None):\n",
    "        self.reset_statistics()\n",
    "        self.al_percentage = al_percentage\n",
    "        self.true_drift_points = true_drift_points\n",
    "        self.retrainsize = retrainsize\n",
    "        self.samplesize = samplesize\n",
    "        self.sparsity = sparsity\n",
    "        self.initial_batch_size = initial_batch_size\n",
    "        self.err_based = err_based\n",
    "        self.approach = approach\n",
    "        self.sampling = sampling\n",
    "                \n",
    "        data_full = data_full\n",
    "        data = data_sparse\n",
    "\n",
    "        # initialize model and prepare data\n",
    "        if multiclass:\n",
    "            clf = xgb.XGBClassifier(objective = 'multi:softprob', num_class = amount_classes)\n",
    "            self.model_type = 'Xg'   \n",
    "        elif multiclass == False and clf is None:\n",
    "            clf = xgb.XGBClassifier()\n",
    "            self.model_type = 'Xg'            \n",
    "        else:\n",
    "            self.model_type = 'Input'\n",
    "            \n",
    "        if sparsity == 100:\n",
    "            stream = DataStream(data = data, allow_nan = True)\n",
    "        else:\n",
    "            stream = DataStream(data = data_full)\n",
    "\n",
    "        # get initial trainig data\n",
    "        x_train, y_train = stream.next_sample(initial_batch_size)\n",
    "        rows, columns = x_train.shape\n",
    "        \n",
    "        # only for initial model training: overwrite unlabaled data with labeled data for initial model training\n",
    "        if sparsity == 100:\n",
    "            x_train = data_full.iloc[:initial_batch_size, :-1].values\n",
    "            y_train = data_full.iloc[:initial_batch_size, -1].values\n",
    "\n",
    "        clf.fit(x_train, y_train)\n",
    "        \n",
    "        # in real world scenarios, we keep the initial training batch for later retraining \n",
    "        if real_world:\n",
    "            x_initial_batch_sample = initial_batch_sample.iloc[:, :-1].values \n",
    "            y_initial_batch_sample = initial_batch_sample.iloc[:, -1].values \n",
    "        \n",
    "        # initial creation of explainer object and calculation of model explanation\n",
    "        if err_based == False:\n",
    "            explainer = self.create_explainer(classifier=clf, data = x_train[-100:], approach = approach) \n",
    "            if not multiclass:\n",
    "                model_shap_distr = explainer.shap_values(x_train).tolist()\n",
    "            else:\n",
    "                model_shap_distr = explainer.shap_values(x_train)\n",
    "\n",
    "        # Set initial values\n",
    "        sample_count = 1\n",
    "        x_storage = np.zeros((0,columns))\n",
    "        y_storage = np.zeros((0,1))\n",
    "        \n",
    "        # detect drift and retrain model \n",
    "        print('start')\n",
    "        for i in range(stream.n_remaining_samples()):\n",
    "            x_test, y_test = stream.next_sample(samplesize)\n",
    "            \n",
    "            #if i%5000 == 0:\n",
    "                #print('it', i)\n",
    "            \n",
    "            if len(x_test) == 0: \n",
    "                break\n",
    "            \n",
    "            # store all batches (labeled and unlabeled instances from stream)\n",
    "            x_storage = np.append(x_storage, x_test, axis=0)\n",
    "            y_storage = np.append(y_storage, y_test)         \n",
    "\n",
    "            # collect explanations\n",
    "            if err_based==False:\n",
    "                if not multiclass:\n",
    "                    shap_values = explainer.shap_values(x_test).tolist()\n",
    "                    if samplesize > 1:\n",
    "                        shap_values = self.aggregate_shap_values(shap_values)\n",
    "                else:\n",
    "                    shap_values = explainer.shap_values(x_test)\n",
    "                    shap_values = self.aggregate_shap_values_multiclass(shap_values)\n",
    "\n",
    "            # collect results\n",
    "            predictions = clf.predict(x_test)\n",
    "            y_test = self.get_labels(data_full, x_test)\n",
    "            \n",
    "            if not multiclass:                \n",
    "                self.prob_predictions.extend(clf.predict_proba(x_test)[:,1]) \n",
    "            else:\n",
    "                self.prob_predictions.extend(clf.predict_proba(x_test)) \n",
    "                \n",
    "                \n",
    "            self.predictions.extend(predictions) \n",
    "            self.true_label_list.extend(y_test)\n",
    "            self.score_list.append(accuracy_score(y_test, predictions)) \n",
    "            self.weighted_f1_list.append(precision_recall_fscore_support(y_test, predictions, average = 'weighted', warn_for=tuple())[2])\n",
    "            \n",
    "            if err_based==False:\n",
    "                self.shap_list.append(shap_values[0])\n",
    "                # compute distance\n",
    "                dist = self.compute_distance(shap_values, model_shap_distr, distance_measure, multiclass, columns)\n",
    "                # use distance\n",
    "                self.base_detector.add_element(dist)\n",
    "                self.distances.append(dist)\n",
    "                \n",
    "            elif self.err_based:\n",
    "                #print(accuracy_score(y_test, predictions))\n",
    "                self.base_detector.add_element(accuracy_score(y_test, predictions))\n",
    "                \n",
    "            sample_count += 1\n",
    "\n",
    "            # trigger retraining because of concept drift\n",
    "            if i in fix_drifts:\n",
    "                self.drift_detections.append(i)\n",
    "                print('Drift, No. of iterations:',  i, 'Samples: ', i*samplesize)\n",
    "\n",
    "                # get labels for unlabeled instances by active learning\n",
    "                x_labeled, x_unlabeled, y_labeled, y_unlabeled = self.filter_missing(x_storage[-retrainsize:], y_storage[-retrainsize:])       \n",
    "                \n",
    "                # In case of the KSWIN scenario with fix predetermined drift points, corresponding parameters\n",
    "                # are assigned a corresponding value, which prevents this method from being started\n",
    "                # since KSWIN is only evaluated in a scenario with 100 % label availability\n",
    "                if len(x_unlabeled) != 0 or sparsity != 0:\n",
    "                    x_labeled, y_labeled = self.active_learning(x_labeled, x_unlabeled, y_labeled, columns, data_full, uncertainty_threshold, amount_classes, clf, sampling, al_percentage) \n",
    "\n",
    "                # in real world scenarios both, 10% of the initial batch and the last x-samples are used together\n",
    "                if real_world:\n",
    "                    x_labeled = np.append(x_labeled, x_initial_batch_sample, axis=0)\n",
    "                    y_labeled = np.append(y_labeled, y_initial_batch_sample)\n",
    "                    \n",
    "                #retrain the model\n",
    "                clf.fit(x_labeled, y_labeled)\n",
    "                \n",
    "                # compute global explanation \n",
    "                if err_based==False:\n",
    "                    explainer = self.create_explainer(classifier=clf, data = x_labeled, approach = approach)\n",
    "                \n",
    "                    if not multiclass:\n",
    "                        model_shap_distr = explainer.shap_values(x_labeled).tolist()\n",
    "                    else:\n",
    "                        model_shap_distr = explainer.shap_values(x_labeled)\n",
    "\n",
    "                sample_count = self.reset_change_parameters() \n",
    "        \n",
    "        # compute performance metrics\n",
    "        self.acc_score = accuracy_score(self.true_label_list, self.predictions)\n",
    "        self.weighted_f1_score = precision_recall_fscore_support(self.true_label_list, self.predictions, average = 'weighted', warn_for=tuple())[2]\n",
    "        self.mcc_score = matthews_corrcoef(self.true_label_list, self.predictions)                \n",
    "        if not multiclass:\n",
    "            self.auc = roc_auc_score(self.true_label_list, self.prob_predictions, average='weighted') \n",
    "        else:\n",
    "            sorted_labels = clf.classes_\n",
    "            self.auc = roc_auc_score(self.true_label_list, self.prob_predictions, average='weighted', multi_class = 'ovo', labels = sorted_labels)\n",
    "\n",
    "        \n",
    "        self.create_export()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def get_statistics(self, drift_range):\n",
    "        statistics = {}\n",
    "        \n",
    "        statistics[\"Model\"] = self.model_type\n",
    "        \n",
    "        # shap detector\n",
    "        statistics[\"Retrainsize\"] = self.retrainsize\n",
    "        statistics[\"Samplesize\"] = self.samplesize\n",
    "        statistics[\"Initial Instances\"] = self.initial_batch_size\n",
    "        statistics[\"Approach\"] = 'Proba' if self.approach == 2 else 'Standard'\n",
    "        \n",
    "        # base detector\n",
    "        statistics[\"Ph Alpha\"] = self.alpha\n",
    "        statistics[\"Ph Delta\"] = self.delta\n",
    "        statistics[\"Ph Min Inst\"] = self.min_instances\n",
    "        statistics[\"Ph Threshold\"] = self.threshold\n",
    "        statistics[\"Ad Delta\"] = self.ad_delta\n",
    "        statistics[\"Ks Alpha\"] = self.ks_alpha\n",
    "        statistics[\"Base Detector\"] = self.base_detector_type\n",
    "                \n",
    "        # scores\n",
    "        statistics[\"Weighted F1\"] = np.round(self.weighted_f1_score, decimals = 3)\n",
    "        statistics[\"Acc\"] = np.round(self.acc_score, decimals = 3)\n",
    "        statistics[\"ROC_AUC\"] = np.round(self.auc, decimals = 3) if self.auc is not None else '-'      \n",
    "        statistics[\"Mcc\"] = np.round(self.mcc_score, decimals = 3)\n",
    "        statistics[\"Detections Count\"] = len(self.drift_detections)\n",
    "        \n",
    "        # drifts        \n",
    "        statistics[\"FAC\"], statistics[\"MDC\"], statistics[\"MDR\"], statistics[\"MTD\"], statistics[\"MTFA\"], statistics[\"MTR\"] = np.round(self.get_drift_metrics(self.drift_detections, self.true_drift_points, drift_range), decimals = 3)\n",
    "        statistics[\"True Drift Points\"] = self.true_drift_points\n",
    "        statistics[\"Triggered Drifts\"] = self.drift_detections\n",
    "                \n",
    "        # error-rate based\n",
    "        statistics[\"Error Based\"] = str(self.err_based)\n",
    "        \n",
    "        #active learning\n",
    "        statistics[\"Sampling\"] = self.sampling\n",
    "\n",
    "        # labels\n",
    "        if self.err_based == True or self.sparsity == 0: # measured by the metric (\"retrainsize\") of the batch for retraining \n",
    "            statistics[\"Labels Retraining %\"] = 100\n",
    "        else:\n",
    "            statistics[\"Labels Retraining %\"] = self.al_percentage\n",
    "\n",
    "        if self.err_based: \n",
    "            statistics[\"Labels Detection %\"] = 100 # Percentage of labels available after the initial training\n",
    "        else:\n",
    "            statistics[\"Labels Detection %\"] = 0\n",
    "\n",
    "        return statistics\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_drift_metrics(drift_detections, true_drift_points, drift_range):\n",
    "        drift_detections.sort()\n",
    "        true_drift_points.sort()\n",
    "        diffs = []\n",
    "        \n",
    "        # MDC params\n",
    "        mdc = 0\n",
    "        \n",
    "        # FAC params\n",
    "        detections = []\n",
    "        false_detections = []\n",
    "        \n",
    "        # FAC - false alarm count\n",
    "        if len(drift_detections) > 0 and len(true_drift_points) > 0:\n",
    "            for idx, td in enumerate(true_drift_points):\n",
    "                if idx+1 < len(true_drift_points):\n",
    "                    n_td = true_drift_points[idx+1]\n",
    "                    # check if drift is detected\n",
    "                    for dd in drift_detections:\n",
    "                        if td <= dd < n_td:\n",
    "                            detections.append(dd)  \n",
    "             \n",
    "                    # check if more than one drift triggered for one true drift and collect false detections if detections are outside of drift_range\n",
    "                    if len(detections) > 1:\n",
    "                        detections.sort()    \n",
    "                        detections.pop(0)\n",
    "                        if len(detections) > 0:\n",
    "                            for dd in detections:\n",
    "                                if dd>(td+drift_range):\n",
    "                                    false_detections.append(dd)   \n",
    "                    detections.clear()\n",
    "        \n",
    "            # detections of first and last true drift \n",
    "            if drift_detections:\n",
    "                # last                                            \n",
    "                for dd in drift_detections:\n",
    "                    if dd > max(true_drift_points):\n",
    "                        detections.append(dd) \n",
    "                if len(detections) > 1:\n",
    "                    detections.sort()    \n",
    "                    detections.pop(0)\n",
    "                    if len(detections) > 0:\n",
    "                        for dd in detections:\n",
    "                            if dd>(max(true_drift_points)+drift_range):\n",
    "                                false_detections.append(dd)                                    \n",
    "                # first                                            \n",
    "                for dd in drift_detections:\n",
    "                    if dd < min(true_drift_points):\n",
    "                        false_detections.append(dd)\n",
    "                        \n",
    "            fac = len(false_detections)\n",
    "            \n",
    "        else:\n",
    "            fac = 0\n",
    "\n",
    "        \n",
    "        # MDC - missed detection count\n",
    "        for idx, td in enumerate(true_drift_points):\n",
    "            found = False\n",
    "            if idx+1 < len(true_drift_points):\n",
    "                n_td = true_drift_points[idx+1]\n",
    "                # check if drift is detected\n",
    "                for dd in drift_detections:\n",
    "                    if td <= dd < n_td:\n",
    "                        found = True\n",
    "                if not found:\n",
    "                    mdc += 1\n",
    "        # did not find any drift    \n",
    "        if len(drift_detections) == 0:\n",
    "            mdc = len(true_drift_points)\n",
    "        # did not find last drift\n",
    "        elif true_drift_points and (max(true_drift_points) > max(drift_detections)):\n",
    "            mdc += 1\n",
    "        \n",
    "        \n",
    "        # MTD - mean time to detection\n",
    "        if len(drift_detections) > 0 and len(true_drift_points) > 0:\n",
    "            for idx, td in enumerate(true_drift_points):\n",
    "                if idx+1 < len(true_drift_points):\n",
    "                    n_td = true_drift_points[idx+1]\n",
    "                    for dd in drift_detections:\n",
    "                        # check if drift is detected\n",
    "                        if td <= dd < n_td:\n",
    "                            diffs.append(dd-td)\n",
    "                            break\n",
    "            # diff last drift\n",
    "            if drift_detections:\n",
    "                for dd in drift_detections:\n",
    "                    if dd > max(true_drift_points):\n",
    "                        diffs.append(dd-td)       \n",
    "                        break\n",
    "            mtd = np.round(np.mean(diffs), decimals = 3) \n",
    "        else:\n",
    "            mtd = 0        \n",
    "        \n",
    "        \n",
    "        # MDR - missed detection rate\n",
    "        if len(true_drift_points) >= 1 and mdc is not None :\n",
    "            mdr = mdc/len(true_drift_points)\n",
    "        else:\n",
    "            mdr = 0\n",
    "\n",
    "            \n",
    "        # MTFA - mean time between false alarms\n",
    "        false_detections.sort()\n",
    "        if false_detections:\n",
    "            mtfa = np.round( np.mean([false_detections[i + 1] - false_detections[i] for i in range(len(false_detections)-1)]), decimals = 3 )\n",
    "        else:\n",
    "            mtfa = 0\n",
    "            \n",
    "\n",
    "        # MTR\n",
    "        if mtd != 0:\n",
    "            mtr = np.round((mtfa/mtd)*(1-mdr), decimals = 3)\n",
    "        else:\n",
    "            mtr = 0\n",
    "                \n",
    "        return fac,mdc,mdr,mtd,mtfa,mtr \n",
    "    \n",
    "    \n",
    "    def create_export(self):\n",
    "        # reduce file size by removing unnecessary class attributes\n",
    "        self.base_detector = None\n",
    "        return\n",
    "              \n",
    "        \n",
    "    def create_base_detector(self, base_detector_type=None, base_detector_config=None):\n",
    "        if base_detector_type is None: \n",
    "            self.base_detector = PageHinkley()\n",
    "            self.base_detector_type = 'ph'\n",
    "            \n",
    "            self.alpha = self.base_detector.alpha\n",
    "            self.delta = self.base_detector.delta\n",
    "            self.min_instances = self.base_detector.min_instances\n",
    "            self.threshold = self.base_detector.threshold\n",
    "            \n",
    "        elif base_detector_config is not None:\n",
    "            if base_detector_type == 'adwin':\n",
    "                self.base_detector = ADWIN()\n",
    "                self.base_detector_type = 'adwin'\n",
    "                self.ad_delta=self.base_detector.delta = base_detector_config[\"ad_delta\"]\n",
    "            \n",
    "            elif base_detector_type == 'kswin':\n",
    "                self.base_detector = KSWIN()\n",
    "                self.base_detector_type = 'kswin'\n",
    "                self.ks_alpha=self.base_detector.alpha = base_detector_config[\"ks_alpha\"]\n",
    "\n",
    "            elif base_detector_type == 'ph':\n",
    "                self.base_detector = PageHinkley()\n",
    "                self.base_detector_type = 'ph'\n",
    "                self.alpha=self.base_detector.alpha = base_detector_config[\"alpha\"]\n",
    "                self.delta=self.base_detector.delta = base_detector_config[\"delta\"]\n",
    "                self.min_instances=self.base_detector.min_instances = base_detector_config[\"min_instances\"]\n",
    "                self.threshold=self.base_detector.threshold = base_detector_config[\"threshold\"]\n",
    "        else:\n",
    "            raise ValueError('If you provide a detector type, you have to provide an appropriate configuration as well!')\n",
    "        \n",
    "        \n",
    "        return self.base_detector \n",
    "        \n",
    "        \n",
    "    def compute_distance(self, shap_values_inst, shap_values_mod, distance_measure, multiclass, columns):\n",
    "        #compute distance between local explanation of an instance and the global explanation of the model                 \n",
    "        if distance_measure == 'euclidean':\n",
    "            if not multiclass:\n",
    "                if len(shap_values_mod) > 1:\n",
    "                    shap_values_mod = self.aggregate_shap_values(shap_values_mod)\n",
    "                diff = self.euclidean_distance(shap_values_inst[0], shap_values_mod[0], columns=columns)\n",
    "            else:\n",
    "                if len(shap_values_mod) > 1:\n",
    "                    shap_values_mod = self.aggregate_shap_values_multiclass(shap_values_mod)\n",
    "                diff = self.euclidean_distance(shap_values_inst[0], shap_values_mod[0], columns=columns)\n",
    "                \n",
    "        elif distance_measure == 'manhattan':\n",
    "            if not multiclass:\n",
    "                if len(shap_values_mod) > 1:\n",
    "                    shap_values_mod = self.aggregate_shap_values(shap_values_mod)\n",
    "                diff = self.manhattan_distance(shap_values_inst[0], shap_values_mod[0]) \n",
    "            else:\n",
    "                if len(shap_values_mod) > 1:\n",
    "                    shap_values_mod = self.aggregate_shap_values_multiclass(shap_values_mod)\n",
    "                diff = self.manhattan_distance(shap_values_inst[0], shap_values_mod[0]) \n",
    "        else:\n",
    "            raise ValueError('Please assign distance measure \"manhattan\" or \"euclidean\"!')\n",
    "        \n",
    "        return diff\n",
    "  \n",
    "\n",
    "    def reset_statistics(self):\n",
    "        self.shap_list = []\n",
    "        self.score_list = []\n",
    "        self.weighted_f1_list = []\n",
    "        self.distances = []\n",
    "        self.base_detector.reset()\n",
    "        self.drift_detections = []  \n",
    "        self.p_values = []\n",
    "        self.prob_predictions = []\n",
    "        self.predictions = []\n",
    "        self.true_label_list = []\n",
    "        \n",
    "        self.true_drift_points = []\n",
    "        self.retrainsize = None\n",
    "        self.samplesize = None\n",
    "        self.al_percentage = None\n",
    "        self.sparsity = None\n",
    "        self.initial_batch_size = None\n",
    "        self.err_based = None\n",
    "        self.approach = None\n",
    "        self.model_type = None\n",
    "        \n",
    "        self.auc = None\n",
    "        self.acc_score = None\n",
    "        self.weighted_f1_score = None\n",
    "        self.mcc_score = None\n",
    "        \n",
    "        self.sampling = None\n",
    "    \n",
    "    def reset_change_parameters(self):\n",
    "        self.base_detector.reset()\n",
    "        sample_count = 1 \n",
    "        \n",
    "        return sample_count\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_indices(data_full, x_labeled): \n",
    "        data_v = data_full.iloc[:,:-1].values\n",
    "        pairwise_compare = data_v == x_labeled[:, np.newaxis, :]\n",
    "        result = pairwise_compare.all(axis=2)\n",
    "        indices = [data_full[l].index[0] for l in result]\n",
    "        \n",
    "        return indices\n",
    "\n",
    "    @staticmethod\n",
    "    def get_labels(data_full, x_unlabeled):\n",
    "        data_v = data_full.iloc[:,:-1].values\n",
    "        pairwise_compare = data_v == x_unlabeled[:, np.newaxis, :]\n",
    "        result = pairwise_compare.all(axis=2)\n",
    "        labels = [data_full[l].iloc[0,-1] for l in result]\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_missing(data, labels):\n",
    "        nan = np.isnan(labels)\n",
    "        x_labeled = data[~nan]\n",
    "        x_unlabeled = data[nan]\n",
    "        y_labeled = labels[~nan]\n",
    "        y_unlabeled = labels[nan]\n",
    "        \n",
    "        return x_labeled, x_unlabeled, y_labeled, y_unlabeled\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_confidence(probas, sampling):\n",
    "        # compute difference between 1st and 2nd highest probability outputs\n",
    "        if sampling == 'margin':\n",
    "            part = np.partition(-probas, 1, axis=1)\n",
    "            margin = - part[:, 0] + part[:, 1]\n",
    "            return margin\n",
    "        # compute entropy of probability outputs\n",
    "        else:\n",
    "            entropy = e(probas.T)\n",
    "            return entropy\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_explainer(classifier, approach, data=None):\n",
    "        if approach == 1:\n",
    "            explainer = shap.TreeExplainer(classifier, feature_perturbation = \"tree_path_dependent\")\n",
    "        elif approach == 2:\n",
    "            explainer = shap.TreeExplainer(classifier, data=data, feature_perturbation = 'interventional', model_output='probability')\n",
    "        else:\n",
    "            raise ValueError('Please assign an approach!')\n",
    "        \n",
    "        return explainer\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def aggregate_shap_values(shap_values):\n",
    "        aggr_shap_vector = []\n",
    "        sv_df = pd.DataFrame(shap_values)\n",
    "        rows, columns = sv_df.shape\n",
    "\n",
    "        for i in range(0,columns):\n",
    "            aggr_shap_vector.append(np.sum(np.abs(sv_df.iloc[:,i]))/len(sv_df))\n",
    "\n",
    "        return [aggr_shap_vector]\n",
    "    \n",
    "    @staticmethod\n",
    "    def aggregate_shap_values_multiclass(shap_values):\n",
    "        shap_values_transposed = list(map(list, zip(*shap_values)))\n",
    "        l = [list(map(list, zip(*s))) for s in shap_values_transposed]\n",
    "        l2 = list(map(list, zip(*l)))\n",
    "        avg = [np.mean(col) for col in l2]\n",
    "        \n",
    "        return [avg]\n",
    "                \n",
    "    @staticmethod\n",
    "    def manhattan_distance(x,y):\n",
    "        \n",
    "        return sum(abs(a-b) for a,b in zip(x,y))\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(x,y, columns):\n",
    "        a = [1]*columns\n",
    "        \n",
    "        return distance.seuclidean(x, y, a)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def make_sparse(label, sparsity):\n",
    "        i = np.random.randint(1,10000)\n",
    "        p = sparsity*100\n",
    "        if i<p:\n",
    "            label = np.nan\n",
    "            return label\n",
    "        else:\n",
    "            return label[0]\n",
    "        \n",
    "    \n",
    "    def random_sample_data(self,x,y, percentage):\n",
    "        df = pd.DataFrame(x)\n",
    "        df['y'] = y\n",
    "\n",
    "        df_elements = df.sample(frac = percentage)  \n",
    "        x = df_elements.iloc[:, :-1].values\n",
    "        y = df_elements.iloc[:, -1].values\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}